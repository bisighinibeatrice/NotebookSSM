{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KqUFHPZQkFd9",
        "iDuxMk_gJTRf",
        "ZEGgHoxvJdw0",
        "6ZPVeS09Jm6y",
        "m_pwO4McsAdL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Some Key Concepts about PCA Implementation\n",
        "\n",
        "For large datasets, computing the covariance matrix and its eigenvalues/eigenvectors can be computationally expensive and inefficient. This inefficiency arises from the quadratic complexity involved in calculating the full covariance matrix with respect to the number of features. In contrast, PCA is typically implemented in a more optimized manner, making it better suited for handling large datasets efficiently.\n",
        "\n",
        "The key distinction between the two approaches lies in the computation of principal components. PCA directly computes a maximum of `min(n_samples, n_features)` components, avoiding the need to compute the full covariance matrix.\n",
        "\n",
        "We can demonstrate that both approaches yield the same principal components through a simple example:"
      ],
      "metadata": {
        "id": "KqUFHPZQkFd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate a random data matrix X (5 samples, 30 features)\n",
        "# Each row in X corresponds to a sample and each column corresponds to a feature.\n",
        "X = np.random.rand(5, 30)\n",
        "\n",
        "# Compute the covariance matrix of the data\n",
        "# The covariance matrix describes the variance and correlation between each pair of features.\n",
        "cov_matrix = np.cov(X.T)\n",
        "\n",
        "# Compute eigenvalues and eigenvectors of the covariance matrix\n",
        "# Eigenvalues represent the variance along each eigenvector (principal component),\n",
        "# and the eigenvectors are the directions of maximum variance.\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "# Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
        "# We want the eigenvectors corresponding to the largest eigenvalues, as they represent the most important principal components.\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "# Print the shapes of the covariance matrix, eigenvalues, and eigenvectors\n",
        "print(\"Shape of covariance matrix:\", cov_matrix.shape)  # Should be (30, 30)\n",
        "print(\"Shape of eigenvalues:\", eigenvalues.shape)        # Should be (30,)\n",
        "print(\"Shape of eigenvectors:\", eigenvectors.shape)      # Should be (30, 30)"
      ],
      "metadata": {
        "id": "xk_iPho3kBxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA\n",
        "# We fit PCA to use it directly on the data instead of the covariance matrix.\n",
        "# PCA will automatically compute the eigenvalues and eigenvectors for us.\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Extract principal components (eigenvectors sorted by explained variance)\n",
        "principal_components = pca.components_\n",
        "\n",
        "# Print the shape of the principal components (similar to eigenvectors)\n",
        "print(\"Shape of principal_components:\", principal_components.shape)  # Should be (n_components, 30)"
      ],
      "metadata": {
        "id": "87ufwbMGYshE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the first eigenvector from the covariance matrix with the first principal component\n",
        "# The eigenvectors and the principal components are related but may differ by sign and order.\n",
        "print(\"Eigenvector from covariance matrix (1st):\", eigenvectors[:, 1])\n",
        "print(\"Principal Component (1st):\", principal_components[1])\n",
        "\n",
        "# Check if the first eigenvector and principal component are the same (up to a sign flip)\n",
        "# They should be the same (ignoring sign) because PCA uses eigenvectors of the covariance matrix.\n",
        "print(\"Are the first eigenvector and principal component the same (up to sign)?\")\n",
        "print(np.allclose(eigenvectors[:, 0], principal_components[0]))"
      ],
      "metadata": {
        "id": "DQ8ZUBIaYwNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Shape Modeling in Biomechanics: Heart Left Ventricle\n",
        "\n",
        "In this notebook, we will learn how to build a a statistical shape model (SSM) of the human left ventricle using Python libraries. The notebook is based on the GitHub repository [Statistical-Shape-Model](https://github.com/UK-Digital-Heart-Project/Statistical-Shape-Model) built within the UK Digital Heart Project, which was set up by Prof. Stuart Cook and Dr. Declan O'Regan at the MRC Clinical Sciences Centre, Imperial College London, UK.\n"
      ],
      "metadata": {
        "id": "FrFndxkD4Uva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDuxMk_gJTRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os  # For interacting with the file system\n",
        "import shutil"
      ],
      "metadata": {
        "id": "7M4-9vdd9mdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PUL2bqOO4AhL"
      },
      "outputs": [],
      "source": [
        "# Setting up the environment\n",
        "os.chdir('/content')\n",
        "\n",
        "# Check if the folder 'NotebookROM' exists before attempting to delete it to reinstall it\n",
        "if os.path.exists('NotebookSSM'):\n",
        "    # Remove the folder and all its contents\n",
        "    shutil.rmtree('NotebookSSM')\n",
        "\n",
        "# Pull the repo with data and install packages for the notebook if we're in Google Colab\n",
        "try:\n",
        "  import google.colab # Check if we're in Google Colab\n",
        "  !sudo apt install libgl1-mesa-glx xvfb\n",
        "  !pip install pyvista[jupyter] -qq\n",
        "  current_directory = os.path.basename(os.getcwd())\n",
        "  # If we're not in the right directory or the directory doesn't exist, clone the repo\n",
        "  if not (os.path.isdir('NotebookSSM') or current_directory == 'NotebookSSM'):\n",
        "    !git clone https://github.com/bisighinibeatrice/NotebookSSM.git\n",
        "  if os.path.isdir('NotebookSSM'):\n",
        "    %cd NotebookSSM\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import vtk  # Library for handling VTK files\n",
        "import numpy as np  # For numerical computations and array manipulations\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "fvjJozCu-Dgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SSM will be constructed using 100 shapes (`M = 100`). These shapes were already registered to the template space using rigid registration (so that the position and orientation differences are removed)."
      ],
      "metadata": {
        "id": "ndm0A8cw59ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, we will read all the shapes that are contained in the `Input` folder and store them in the matrix `X`.\n",
        "\n",
        "> **NOTA BENE:** Contrary from what seen in the theoretical part of the class, the matrix `X` should be organized such that rows represent different shapes (samples) and columns represent features. This is requested by the `PCA` function."
      ],
      "metadata": {
        "id": "X2Wd6u1nRzMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory containing the VTK files\n",
        "vtk_dir = 'Input/'  # Set the appropriate directory where your VTK files are saved\n",
        "\n",
        "# List all VTK files in the directory that include 'sample' in their filenames\n",
        "vtk_files = [f for f in os.listdir(vtk_dir) if f.endswith('.vtk') and 'sample' in f]\n",
        "\n",
        "# Print the number of VTK files found in the specified directory\n",
        "print(f\"Number of VTK files found: {len(vtk_files)}\")\n",
        "\n",
        "# Initialize a list to store all shapes for subsequent analysis (PCA)\n",
        "shapes = []\n",
        "\n",
        "# Process each VTK file: read the data and extract point coordinates\n",
        "for file in vtk_files:\n",
        "    reader = vtk.vtkPolyDataReader()  # Create a reader to handle VTK file format\n",
        "    reader.SetFileName(os.path.join(vtk_dir, file))  # Specify the file to read\n",
        "    reader.Update()  # Parse the file and load data into the reader\n",
        "\n",
        "    # Extract the geometry (polydata) from the file\n",
        "    polydata = reader.GetOutput()\n",
        "    points = polydata.GetPoints()  # Access the points in the polydata\n",
        "    n_points = points.GetNumberOfPoints()  # Count the number of points in the dataset\n",
        "\n",
        "    # Flatten the 3D coordinates of all points into a single vector\n",
        "    shape = []  # Temporary storage for current shape's flattened coordinates\n",
        "    for i in range(n_points):\n",
        "        shape.extend(points.GetPoint(i))  # Add [x, y, z] of each point to the shape vector\n",
        "\n",
        "    # Append the flattened shape vector to the list of all shapes\n",
        "    shapes.append(shape)\n",
        "\n",
        "# Convert the list of shapes into a NumPy matrix (2D array)\n",
        "X = np.array(shapes)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iQ3ECVs45GBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many features is each shape composed of (or equivalently, how much is `N`)? How many points compose each shape?"
      ],
      "metadata": {
        "id": "2zFOZLF4_Lkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of features\n",
        "print(f\"Number of features (coordinates): {X.shape[1]}\")\n",
        "print(f\"Number of points: {X.shape[1]//3}\")"
      ],
      "metadata": {
        "id": "r0LsE0Hu_Mik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "ZEGgHoxvJdw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise these shapes as point-clouds using `Plotly` for interactive 3D visualization."
      ],
      "metadata": {
        "id": "CG4fOl1D_9ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read points from a VTK file\n",
        "def read_vtk_points(vtk_file_path):\n",
        "    \"\"\"\n",
        "    Read points (vertices) from a VTK file.\n",
        "\n",
        "    Parameters:\n",
        "        vtk_file_path (str): Full path to the VTK file.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of shape (n_points, 3) containing the 3D coordinates.\n",
        "    \"\"\"\n",
        "    # Read the VTK file\n",
        "    reader = vtk.vtkPolyDataReader()\n",
        "    reader.SetFileName(vtk_file_path)\n",
        "    reader.Update()\n",
        "\n",
        "    # Extract points (vertices) from the polydata\n",
        "    polydata = reader.GetOutput()\n",
        "    points = polydata.GetPoints()\n",
        "    n_points = points.GetNumberOfPoints()\n",
        "\n",
        "    # Convert points to a numpy array\n",
        "    numpy_points = np.array([points.GetPoint(i) for i in range(n_points)])\n",
        "\n",
        "    return numpy_points\n",
        "\n",
        "# Function to visualize multiple VTK meshes interactively using Plotly\n",
        "def visualize_vtk_meshes_plotly(mesh_points_list, mesh_names):\n",
        "    \"\"\"\n",
        "    Visualize the meshes interactively using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "        mesh_points_list (list of numpy.ndarray): List of numpy arrays, each containing the points for a mesh.\n",
        "        mesh_names (list of str): List of names for each mesh (e.g., file names).\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, points in enumerate(mesh_points_list):\n",
        "        # Add the points from this mesh as a trace in the plot\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=points[:, 0],\n",
        "            y=points[:, 1],\n",
        "            z=points[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=2, opacity=0.8),\n",
        "            name=mesh_names[i]  # Use the file name as the legend\n",
        "        ))\n",
        "\n",
        "    # Set plot layout\n",
        "    fig.update_layout(\n",
        "        title=\"3D Shape Visualization (Superimposed)\",\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X\",\n",
        "            yaxis_title=\"Y\",\n",
        "            zaxis_title=\"Z\"\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rju0oUV2Bpzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can read one of the point clouds using `read_vtk_points` and visualize it with `visualize_vtk_meshes_plotly`."
      ],
      "metadata": {
        "id": "k1ybyxhq37ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Read points from VTK file\n",
        "vtk_file_paths = [\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_1.vtk\")\n",
        "]\n",
        "\n",
        "# Read points from all VTK files\n",
        "mesh_points_list = [read_vtk_points(file_path) for file_path in vtk_file_paths]\n",
        "mesh_names = [os.path.basename(file_path) for file_path in vtk_file_paths]\n",
        "\n",
        "# Visualize the meshes superimposed\n",
        "print(f\"Visualizing files: {', '.join(mesh_names)}\")\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "6bVVTb1ca3VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also superimpose two point clouds."
      ],
      "metadata": {
        "id": "Scxljl24aCqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Read points from multiple VTK files\n",
        "vtk_file_paths = [\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_55.vtk\"),\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_71.vtk\"),\n",
        "]\n",
        "\n",
        "# Read points from all VTK files\n",
        "mesh_points_list = [read_vtk_points(file_path) for file_path in vtk_file_paths]\n",
        "mesh_names = [os.path.basename(file_path) for file_path in vtk_file_paths]\n",
        "\n",
        "# Visualize the meshes superimposed\n",
        "print(f\"Visualizing files: {', '.join(mesh_names)}\")\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "_7yQ7-2kaB94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis"
      ],
      "metadata": {
        "id": "6ZPVeS09Jm6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PCA identifies the principal components of the data by analyzing its structure and finding the directions of maximum variance.\n",
        "\n",
        "If the data is not centered (i.e., it has a non-zero mean), the analysis may reflect the overall offset instead of the true patterns of variation. By centering the data (subtracting the mean), we ensure that the PCA focuses on the intrinsic variation in the data, ignoring any global shifts. This allows  the PCA to identify the directions of maximum variability effectively."
      ],
      "metadata": {
        "id": "Cfoo9MnXCTYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the mean shape within the database."
      ],
      "metadata": {
        "id": "ITMW3BikbXz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean shape (mean vector across all samples)\n",
        "X_mean  = np.mean(X, axis=0)\n",
        "print(f\"X_mean dimensions: {X_mean .shape}\")"
      ],
      "metadata": {
        "id": "GPaUSO_eCS97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can cen all the shapes with respect to the computed mean shape."
      ],
      "metadata": {
        "id": "zKtZyeopD9mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtract the mean shape from all shapes (center the data)\n",
        "X_centered  = X - X_mean\n",
        "print(f\"X_centered dimensions: {X_centered.shape}\")"
      ],
      "metadata": {
        "id": "K7O98jnxEE8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build the PCA object and fit it with our data."
      ],
      "metadata": {
        "id": "Dbtlzx84k9JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA\n",
        "pca = PCA()\n",
        "# Fit the model to the data\n",
        "pca.fit(X_centered)"
      ],
      "metadata": {
        "id": "lD6nTniwCkNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The principal modes (or components) represent the directions in which the data varies most. They are stored in the `components_` attribute of the fitted PCA object. Each row of `components_` corresponds to a principal mode (or component)."
      ],
      "metadata": {
        "id": "tf4J9JvVKx4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the principal modes (components)\n",
        "principal_components = pca.components_.T  # Transpose to match point-wise interpretation"
      ],
      "metadata": {
        "id": "UuSxg9BrDz6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many principal compoenent do we have? What is the dimension of  each principal components?"
      ],
      "metadata": {
        "id": "i6QGolj3N46a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dimensions of each principal component\n",
        "print(\"Number of principal components:\", principal_components.shape[1])\n",
        "print(\"Dimension of each principal component:\", principal_components.shape[0])"
      ],
      "metadata": {
        "id": "8zSNBdG4OJhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the cumulative variance explained by each principal component in PCA, you can use the `explained_variance_ratio_` attribute from the fitted PCA model. This attribute contains the proportion of variance explained by each principal component. The cumulative variance can be computed by taking the cumulative sum of these values (`cumsum`)."
      ],
      "metadata": {
        "id": "C2dx5fCmMZEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get explained variance ratio (how much variance each principal component explains)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Compute cumulative variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot cumulative variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel(\"Index of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B3BksPBiLAnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine how many principal components are needed to explain 90%, 95%, and 99% of the variance, we can use the cumulative variance plot and check at which point the cumulative variance exceeds these thresholds.\n"
      ],
      "metadata": {
        "id": "npcfiYXsM4mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the number of components for 90%, 95%, and 99% variance explained\n",
        "components_90 = np.argmax(cumulative_variance >= 0.90) + 1  # +1 because index is zero-based\n",
        "components_95 = np.argmax(cumulative_variance >= 0.95) + 1  # +1 because index is zero-based\n",
        "components_99 = np.argmax(cumulative_variance >= 0.99) + 1  # +1 because index is zero-based\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of components to explain 90% variance: {components_90}\")\n",
        "print(f\"Number of components to explain 95% variance: {components_95}\")\n",
        "print(f\"Number of components to explain 99% variance: {components_99}\")"
      ],
      "metadata": {
        "id": "xHsflHN1M4R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cumulative variance for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axvline(x=components_90, color='r', linestyle='--', label=\"90% Variance\")\n",
        "plt.axvline(x=components_95, color='g', linestyle='--', label=\"95% Variance\")\n",
        "plt.axvline(x=components_99, color='y', linestyle='--', label=\"99% Variance\")\n",
        "plt.title(\"Cumulative Explained Variance\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9746SL0jb96M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting a PCA model to your data, we can compute the coefficients by using the transform method of the PCA object. This method projects the original data onto the principal components, effectively computing the coefficients (i.e., the contributions of each principal component for each data point)."
      ],
      "metadata": {
        "id": "lBe79VpLNzH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients (also known as the projection onto the principal components)\n",
        "principal_coefficients = pca.transform(X_centered)\n",
        "\n",
        "# Print the shape of the coefficients matrix\n",
        "print(\"Shape of coefficients matrix:\", principal_coefficients.shape)"
      ],
      "metadata": {
        "id": "lFm0wNYzNxsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconstruction\n",
        "Now, we can reconstruct one of the original shapes by using all the principal components obtained from PCA. The idea is to combine these components with their respective weights to approximate the original shape as closely as possible.\n",
        "\n",
        "Once we have the reconstructed shape, we compare it to the original shape to measure how well the reconstruction captures the original details. To do this, we use the Mean Squared Error (MSE). The MSE quantifies the average squared difference between the original and reconstructed values.\n",
        "\n",
        "A lower MSE indicates a better reconstruction, meaning the reconstructed shape is very close to the original. If the MSE is high, it suggests that some important details might have been lost during the reconstruction. This process helps us evaluate how effective PCA is at preserving the essential features of the data."
      ],
      "metadata": {
        "id": "m_pwO4McsAdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start by using all the principal components."
      ],
      "metadata": {
        "id": "RzmDn3yrAL-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the first shape for reconstruction\n",
        "first_shape = X[0]\n",
        "\n",
        "# Reconstruct the first shape using the all the components\n",
        "reconstructed_shape = np.dot(principal_coefficients[0,], principal_components[:, :].T) + X_mean\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "print(\"\\nReconstruction Error:\")\n",
        "print(mse_error)"
      ],
      "metadata": {
        "id": "pBpQ01LdTZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now reconstruct the first shape using only a subset of the principal components, rather than all of them."
      ],
      "metadata": {
        "id": "z8PmvAJvAiqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of principal components to use\n",
        "num_components_to_use = 10\n",
        "\n",
        "# Use the first shape for reconstruction\n",
        "first_shape = X[0]\n",
        "\n",
        "# Reconstruct the first shape using only the selected number of components\n",
        "reconstructed_shape = (\n",
        "    np.dot(principal_coefficients[0, :num_components_to_use], principal_components[:, :num_components_to_use].T) + X_mean\n",
        ")\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "print(\"\\nReconstruction Error using\", num_components_to_use, \"components:\")\n",
        "print(mse_error)"
      ],
      "metadata": {
        "id": "HlZbtVxwcr2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can modify the code to iteratively increase the number of principal components used for reconstruction, starting from 0 and incrementing by steps of 10, up to the maximum number of components. For each iteration, we can compute and record the reconstruction error (using Mean Squared Error, MSE) to observe how the error decreases as more components are included."
      ],
      "metadata": {
        "id": "Wg1bbl5Im0wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store MSE values\n",
        "mse_errors = []\n",
        "\n",
        "# Loop over 1, 5, and 10 principal components to reconstruct the shape\n",
        "for n_components in range(1, 99, 10):  # n_components ranges from 1 to 50 with step 5\n",
        "    # Reconstruct the first shape using the first n_components\n",
        "    reconstructed_shape = np.dot(principal_coefficients[0, :n_components], principal_components[:, :n_components].T) + X_mean\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "    mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "\n",
        "     # Store the MSE for plotting\n",
        "    mse_errors.append(mse_error)\n",
        "\n",
        "    # Print the MSE for the current reconstruction\n",
        "    print(f\"\\nReconstruction Error with {n_components} components:\")\n",
        "    print(f\"MSE: {mse_error}\")\n",
        "\n",
        "# Plotting the reconstruction errors\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, 99, 10), mse_errors, marker='o', linestyle='-', color='b', label='Reconstruction Error (MSE)')\n",
        "plt.title('Reconstruction Error vs. Number of Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1p3vVkooAnGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can reconstructs the shapes using a specified number of principal components and then superimose the original and reconstructed shapes."
      ],
      "metadata": {
        "id": "KcVbfLSpdlc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the shape index and PCA settings\n",
        "which_shape = 0\n",
        "n_components = 5\n",
        "\n",
        "# Read the original shape from the VTK file\n",
        "original_shape_flat = X[which_shape]  # Access the shape data (e.g., X[0] for the first shape)\n",
        "\n",
        "# Reshape the original shape into a 3D point format (assuming 3D points: x, y, z)\n",
        "n_points = original_shape_flat.shape[0] // 3  # Assuming each shape has 3D points (x, y, z)\n",
        "original_shape = original_shape_flat.reshape(n_points, 3)\n",
        "\n",
        "# Perform PCA reconstruction using the selected number of components\n",
        "reconstructed_shape_flat = np.dot(principal_coefficients[which_shape, :n_components], principal_components[:, :n_components].T) + X_mean\n",
        "\n",
        "# Reshape the reconstructed data back into a 3D point format\n",
        "n_points = reconstructed_shape_flat.shape[0] // 3  # Reshape back to 3D coordinates\n",
        "reconstructed_shape = reconstructed_shape_flat.reshape(n_points, 3)\n",
        "\n",
        "# Compute MSE between original (from VTK) and reconstructed shape\n",
        "mse_error = np.mean((original_shape_flat - reconstructed_shape_flat) ** 2)\n",
        "print(f\"\\nReconstruction Error (MSE) using {n_components} components:\")\n",
        "print(mse_error)\n",
        "\n",
        "# Visualize original and reconstructed meshes\n",
        "mesh_points_list = [original_shape, reconstructed_shape]\n",
        "mesh_names = [\"Original Shape\", \"Reconstructed Shape\"]\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "wmDXx70wf2g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modes explanation\n",
        "Each mode represents a different direction of variation in the shape, and by adjusting the strength of this variation, we can see how the shape changes."
      ],
      "metadata": {
        "id": "izvaV4Wjc687"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_mode = 0\n",
        "\n",
        "# Read the original shape from the VTK file\n",
        "mean_shape_flat = X_mean\n",
        "\n",
        "# Reshape the original shape into a 3D point format (assuming 3D points: x, y, z)\n",
        "n_points = mean_shape_flat.shape[0] // 3  # Assuming each shape has 3D points (x, y, z)\n",
        "mean_shape = mean_shape_flat.reshape(n_points, 3)\n",
        "\n",
        "# Calculate the standard deviation of the coefficients for the selected mode\n",
        "# principal_coefficients should be the coefficients from the PCA (this is usually X @ principal_components)\n",
        "mode_std = np.std(principal_coefficients[:, selected_mode])  # Standard deviation of the coefficients for the selected mode\n",
        "print(f\"Standard deviation of the selected mode: {mode_std}\")\n",
        "\n",
        "# Define the coefficient to multiply by the standard deviation to adjust the strength of the modification\n",
        "coeff = 3.0  # Coefficient for scaling the standard deviation (this is user-controlled)\n",
        "\n",
        "# Perform PCA reconstruction using the selected number of components\n",
        "effect_selcted_mode_pos_flat = X_mean + coeff * mode_std * principal_components[:, selected_mode] # Scale the mode's effect\n",
        "effect_selcted_mode_neg_flat = X_mean - coeff * mode_std * principal_components[:, selected_mode]  # Scale the mode's effect\n",
        "\n",
        "# Reshape the reconstructed data back into a 3D point format\n",
        "n_points = effect_selcted_mode_pos_flat.shape[0] // 3  # Reshape back to 3D coordinates\n",
        "effect_selcted_mode_pos = effect_selcted_mode_pos_flat.reshape(n_points, 3)\n",
        "n_points = effect_selcted_mode_neg_flat.shape[0] // 3  # Reshape back to 3D coordinates\n",
        "effect_selcted_mode_neg = effect_selcted_mode_neg_flat.reshape(n_points, 3)\n",
        "\n",
        "# Visualize original and reconstructed meshes\n",
        "mesh_points_list = [original_shape, effect_selcted_mode_pos, effect_selcted_mode_neg]\n",
        "mesh_names = [\"Original Shape\", \"Positive Effect\", \"Negative Effect\"]\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "9dgO1FHLDZQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![Modes](https://i.imgur.com/l9IN8NQ.png)\n",
        "\n",
        "![Modes](https://i.imgur.com/qLT9EgK.png)\n"
      ],
      "metadata": {
        "id": "lgH_1uddZdQp"
      }
    }
  ]
}