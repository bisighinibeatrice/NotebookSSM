{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Some Key Concepts about PCA Implementation\n",
        "\n",
        "For large datasets, computing the covariance matrix and its eigenvalues/eigenvectors can be computationally expensive and inefficient. This inefficiency arises from the quadratic complexity involved in calculating the full covariance matrix with respect to the number of features. In contrast, PCA is typically implemented in a more optimized manner, making it better suited for handling large datasets efficiently.\n",
        "\n",
        "The key distinction between the two approaches lies in the computation of principal components. PCA directly computes a maximum of `min(n_samples, n_features)` components, avoiding the need to compute the full covariance matrix.\n",
        "\n",
        "We can demonstrate that both approaches yield the same principal components through a simple example:"
      ],
      "metadata": {
        "id": "KqUFHPZQkFd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate a random data matrix X (5 samples, 30 features)\n",
        "# Each row in X corresponds to a sample, and each column corresponds to a feature.\n",
        "X = np.random.rand(5, 30)\n",
        "\n",
        "# Compute the covariance matrix of the data\n",
        "# The covariance matrix describes the variance and correlation between each pair of features.\n",
        "cov_matrix = np.cov(X.T)\n",
        "\n",
        "# Compute eigenvalues and eigenvectors of the covariance matrix\n",
        "# Eigenvalues represent the variance along each eigenvector (principal component),\n",
        "# and the eigenvectors are the directions of maximum variance.\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "# Sort eigenvalues and eigenvectors in descending order of eigenvalues\n",
        "# We want the eigenvectors corresponding to the largest eigenvalues, as they represent the most important principal components.\n",
        "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[sorted_indices]\n",
        "eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "# Print the shapes of the covariance matrix, eigenvalues, and eigenvectors\n",
        "print(\"Shape of covariance matrix:\", cov_matrix.shape)  # Should be (30, 30)\n",
        "print(\"Shape of eigenvalues:\", eigenvalues.shape)        # Should be (30,)\n",
        "print(\"Shape of eigenvectors:\", eigenvectors.shape)      # Should be (30, 30)"
      ],
      "metadata": {
        "id": "xk_iPho3kBxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd339aa-0b54-4dd1-f415-5859df3fe7b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of covariance matrix: (30, 30)\n",
            "Shape of eigenvalues: (30,)\n",
            "Shape of eigenvectors: (30, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize PCA\n",
        "# We fit PCA  to use it directly on the data instead of the covariance matrix.\n",
        "# PCA will automatically compute the eigenvalues and eigenvectors for us.\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Extract principal components (eigenvectors sorted by explained variance)\n",
        "principal_components = pca.components_\n",
        "\n",
        "# Print the shape of the principal components (similar to eigenvectors)\n",
        "print(\"Shape of principal_components:\", principal_components.shape)  # Should be (n_components, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ufwbMGYshE",
        "outputId": "3858d7e4-68c4-4827-c9ba-70af3c463afd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of principal_components: (5, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the first eigenvector from the covariance matrix with the first principal component\n",
        "# The eigenvectors and the principal components are related but may differ by sign and order.\n",
        "print(\"Eigenvector from covariance matrix (1st):\", eigenvectors[:, 1])\n",
        "print(\"Principal Component (1st):\", principal_components[1])\n",
        "\n",
        "# Check if the first eigenvector and principal component are the same (up to a sign flip)\n",
        "# They should be the same (ignoring sign) because PCA uses eigenvectors of the covariance matrix.\n",
        "print(\"Are the first eigenvector and principal component the same (up to sign)?\")\n",
        "print(np.allclose(eigenvectors[:, 0], principal_components[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ8ZUBIaYwNH",
        "outputId": "029b9f90-d8e3-407d-c20c-f672e0fd587d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvector from covariance matrix (1st): [-0.16258416 -0.07852845  0.17924357 -0.28292078 -0.08210954  0.26924343\n",
            " -0.16444566  0.01890664  0.1248284  -0.11732894  0.19543674 -0.15028302\n",
            "  0.36034852 -0.08922661 -0.22754581 -0.23135464  0.13621884 -0.02345414\n",
            "  0.09911534 -0.45259354 -0.04554773 -0.07469588 -0.21727005 -0.13075725\n",
            " -0.0563628  -0.29737131 -0.01879679  0.08315625  0.06233544  0.08283421]\n",
            "Principal Component (1st): [ 0.16258416  0.07852845 -0.17924357  0.28292078  0.08210954 -0.26924343\n",
            "  0.16444566 -0.01890664 -0.1248284   0.11732894 -0.19543674  0.15028302\n",
            " -0.36034852  0.08922661  0.22754581  0.23135464 -0.13621884  0.02345414\n",
            " -0.09911534  0.45259354  0.04554773  0.07469588  0.21727005  0.13075725\n",
            "  0.0563628   0.29737131  0.01879679 -0.08315625 -0.06233544 -0.08283421]\n",
            "Are the first eigenvector and principal component the same (up to sign)?\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Shape Modeling in Biomechanics: Heart Left Ventricle\n",
        "\n",
        "In this notebook, we will learn how to build a a statistical shape model (SSM) of the human left ventricle using Python libraries. The notebook is based on the GitHub repository [Statistical-Shape-Model](https://github.com/UK-Digital-Heart-Project/Statistical-Shape-Model) built within the UK Digital Heart Project, which was set up by Prof. Stuart Cook and Dr. Declan O'Regan at the MRC Clinical Sciences Centre, Imperial College London, UK.\n"
      ],
      "metadata": {
        "id": "FrFndxkD4Uva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDuxMk_gJTRf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PUL2bqOO4AhL",
        "outputId": "94d70940-2d88-445e-bfa8-a83ef3c8d5dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libgl1-mesa-glx libxfont2 libxkbfile1 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 10 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 7,820 kB of archives.\n",
            "After this operation, 12.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 7,820 kB in 3s (2,897 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../1-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../2-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../3-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../4-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../5-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../6-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../7-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../8-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../9-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.5/236.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m748.6/748.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'NotebookSSM'...\n",
            "remote: Enumerating objects: 103, done.\u001b[K\n",
            "remote: Counting objects: 100% (103/103), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 103 (delta 96), reused 103 (delta 96), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (103/103), 24.82 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "Updating files: 100% (100/100), done.\n",
            "/content/NotebookSSM\n"
          ]
        }
      ],
      "source": [
        "# Setting up the environment\n",
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Check if the folder 'NotebookROM' exists before attempting to delete it to reinstall it\n",
        "if os.path.exists('NotebookSSM'):\n",
        "    # Remove the folder and all its contents\n",
        "    shutil.rmtree('NotebookSSM')\n",
        "\n",
        "# Pull the repo with data and install packages for the notebook if we're in Google Colab\n",
        "import os\n",
        "try:\n",
        "  import google.colab # Check if we're in Google Colab\n",
        "  !sudo apt install libgl1-mesa-glx xvfb\n",
        "  !pip install pyvista[jupyter] -qq\n",
        "  current_directory = os.path.basename(os.getcwd())\n",
        "  # If we're not in the right directory or the directory doesn't exist, clone the repo\n",
        "  if not (os.path.isdir('NotebookSSM') or current_directory == 'NotebookSSM'):\n",
        "    !git clone https://github.com/bisighinibeatrice/NotebookSSM.git\n",
        "  if os.path.isdir('NotebookSSM'):\n",
        "    %cd NotebookSSM\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SSM will be constructed using 100 shapes (`M = 100`). These shapes were already registered to the template space using rigid registration (so that the position and orientation differences are removed)."
      ],
      "metadata": {
        "id": "ndm0A8cw59ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code, we will read all the shapes that are contained in the `Input` folder adn store them in the matrix `X`.\n",
        "\n",
        "> **NOTA BENE:** Contrary from what seen in the theoretical part of the class, the matrix `X` should be organized such that rows represent different shapes (samples) and columns represent features. This is requested by the `PCA` function."
      ],
      "metadata": {
        "id": "X2Wd6u1nRzMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import vtk  # Library for handling VTK files\n",
        "import numpy as np  # For numerical computations and array manipulations\n",
        "import os  # For interacting with the file system\n",
        "\n",
        "# Directory containing the VTK files\n",
        "vtk_dir = 'Input/'  # Set the appropriate directory where your VTK files are saved\n",
        "\n",
        "# List all VTK files in the directory that include 'sample' in their filenames\n",
        "vtk_files = [f for f in os.listdir(vtk_dir) if f.endswith('.vtk') and 'sample' in f]\n",
        "\n",
        "# Print the number of VTK files found in the specified directory\n",
        "print(f\"Number of VTK files found: {len(vtk_files)}\")\n",
        "\n",
        "# Initialize a list to store all shapes for subsequent analysis (PCA)\n",
        "shapes = []\n",
        "\n",
        "# Process each VTK file: read the data and extract point coordinates\n",
        "for file in vtk_files:\n",
        "    reader = vtk.vtkPolyDataReader()  # Create a reader to handle VTK file format\n",
        "    reader.SetFileName(os.path.join(vtk_dir, file))  # Specify the file to read\n",
        "    reader.Update()  # Parse the file and load data into the reader\n",
        "\n",
        "    # Extract the geometry (polydata) from the file\n",
        "    polydata = reader.GetOutput()\n",
        "    points = polydata.GetPoints()  # Access the points in the polydata\n",
        "    n_points = points.GetNumberOfPoints()  # Count the number of points in the dataset\n",
        "\n",
        "    # Flatten the 3D coordinates of all points into a single vector\n",
        "    shape = []  # Temporary storage for current shape's flattened coordinates\n",
        "    for i in range(n_points):\n",
        "        shape.extend(points.GetPoint(i))  # Add [x, y, z] of each point to the shape vector\n",
        "\n",
        "    # Append the flattened shape vector to the list of all shapes\n",
        "    shapes.append(shape)\n",
        "\n",
        "# Convert the list of shapes into a NumPy matrix (2D array)\n",
        "X = np.array(shapes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iQ3ECVs45GBE",
        "outputId": "fbf1b0e0-17ce-4ae9-e9a6-73f7c54eea5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of VTK files found: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** How many features is each shape composed of (or equivalently, how much is `N`)? How many points compose each shape? **bold text**"
      ],
      "metadata": {
        "id": "2zFOZLF4_Lkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of features\n",
        "print(f\"Number of features (N): {X.shape[1]}\")\n",
        "print(f\"Number of points: {X.shape[1]//3}\")"
      ],
      "metadata": {
        "id": "r0LsE0Hu_Mik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation"
      ],
      "metadata": {
        "id": "ZEGgHoxvJdw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualise these shapes as point-clouds using `Plotly` for interactive 3D visualization."
      ],
      "metadata": {
        "id": "CG4fOl1D_9ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import vtk\n",
        "import numpy as np\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Directory containing the VTK files\n",
        "vtk_dir = 'Input/'  # Adjust this path accordingly\n",
        "\n",
        "# List all VTK files in the directory\n",
        "vtk_files = [f for f in os.listdir(vtk_dir) if f.endswith('.vtk') and 'sample' in f]\n",
        "\n",
        "# Print the number of VTK files found\n",
        "print(f\"Number of VTK files found: {len(vtk_files)}\")\n",
        "\n",
        "# Function to read points from a VTK file\n",
        "def read_vtk_points(vtk_file_path):\n",
        "    \"\"\"\n",
        "    Read points (vertices) from a VTK file.\n",
        "\n",
        "    Parameters:\n",
        "        vtk_file_path (str): Full path to the VTK file.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Array of shape (n_points, 3) containing the 3D coordinates.\n",
        "    \"\"\"\n",
        "    # Read the VTK file\n",
        "    reader = vtk.vtkPolyDataReader()\n",
        "    reader.SetFileName(vtk_file_path)\n",
        "    reader.Update()\n",
        "\n",
        "    # Extract points (vertices) from the polydata\n",
        "    polydata = reader.GetOutput()\n",
        "    points = polydata.GetPoints()\n",
        "    n_points = points.GetNumberOfPoints()\n",
        "\n",
        "    # Convert points to a numpy array\n",
        "    numpy_points = np.array([points.GetPoint(i) for i in range(n_points)])\n",
        "\n",
        "    return numpy_points\n",
        "\n",
        "# Function to visualize multiple VTK meshes interactively using Plotly\n",
        "def visualize_vtk_meshes_plotly(mesh_points_list, mesh_names):\n",
        "    \"\"\"\n",
        "    Visualize the meshes interactively using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "        mesh_points_list (list of numpy.ndarray): List of numpy arrays, each containing the points for a mesh.\n",
        "        mesh_names (list of str): List of names for each mesh (e.g., file names).\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for i, points in enumerate(mesh_points_list):\n",
        "        # Add the points from this mesh as a trace in the plot\n",
        "        fig.add_trace(go.Scatter3d(\n",
        "            x=points[:, 0],\n",
        "            y=points[:, 1],\n",
        "            z=points[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=2, opacity=0.8),\n",
        "            name=mesh_names[i]  # Use the file name as the legend\n",
        "        ))\n",
        "\n",
        "    # Set plot layout\n",
        "    fig.update_layout(\n",
        "        title=\"3D Shape Visualization (Superimposed)\",\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X\",\n",
        "            yaxis_title=\"Y\",\n",
        "            zaxis_title=\"Z\"\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Rju0oUV2Bpzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Read points from multiple VTK files\n",
        "vtk_file_paths = [\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_1.vtk\")\n",
        "]\n",
        "\n",
        "# Read points from all VTK files\n",
        "mesh_points_list = [read_vtk_points(file_path) for file_path in vtk_file_paths]\n",
        "mesh_names = [os.path.basename(file_path) for file_path in vtk_file_paths]\n",
        "\n",
        "# Visualize the meshes superimposed\n",
        "print(f\"Visualizing files: {', '.join(mesh_names)}\")\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "6bVVTb1ca3VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** Choose two case and visualize the meshes separately, followed by a superimposed view of both."
      ],
      "metadata": {
        "id": "Scxljl24aCqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Read points from multiple VTK files\n",
        "vtk_file_paths = [\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_55.vtk\"),\n",
        "    os.path.join(vtk_dir, \"LV_ED_sample_71.vtk\"),\n",
        "]\n",
        "\n",
        "# Read points from all VTK files\n",
        "mesh_points_list = [read_vtk_points(file_path) for file_path in vtk_file_paths]\n",
        "mesh_names = [os.path.basename(file_path) for file_path in vtk_file_paths]\n",
        "\n",
        "# Visualize the meshes superimposed\n",
        "print(f\"Visualizing files: {', '.join(mesh_names)}\")\n",
        "visualize_vtk_meshes_plotly(mesh_points_list, mesh_names)"
      ],
      "metadata": {
        "id": "_7yQ7-2kaB94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis"
      ],
      "metadata": {
        "id": "6ZPVeS09Jm6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PCA identifies the principal components of the data by analyzing its structure and finding the directions of maximum variance.\n",
        "\n",
        "If the data is not centered (i.e., it has a non-zero mean), the analysis may reflect the overall offset instead of the true patterns of variation. By centering the data (subtracting the mean), we ensure that the PCA focuses on the intrinsic variation in the data, ignoring any global shifts. This allows  the PCA to identify the directions of maximum variability effectively."
      ],
      "metadata": {
        "id": "Cfoo9MnXCTYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** Compute the mean shape within the database."
      ],
      "metadata": {
        "id": "ITMW3BikbXz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean shape (mean vector across all samples)\n",
        "X_mean  = np.mean(X, axis=0)\n",
        "print(f\"X_mean dimensions: {X_mean .shape}\")"
      ],
      "metadata": {
        "id": "GPaUSO_eCS97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** Center all the shapes with respect to the just-computed mean shape."
      ],
      "metadata": {
        "id": "zKtZyeopD9mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtract the mean shape from all shapes (center the data)\n",
        "X_centered  = X - X_mean\n",
        "print(f\"X_centered dimensions: {X_centered.shape}\")"
      ],
      "metadata": {
        "id": "K7O98jnxEE8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build the PCA object and fit it with our data."
      ],
      "metadata": {
        "id": "Dbtlzx84k9JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize PCA\n",
        "pca = PCA()\n",
        "# Fit the model to the data\n",
        "pca.fit(X_centered)"
      ],
      "metadata": {
        "id": "lD6nTniwCkNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The principal modes (or components) represent the directions in which the data varies most. They are stored in the `components_` attribute of the fitted PCA object. Each row of `components_` corresponds to a principal mode (or component)."
      ],
      "metadata": {
        "id": "tf4J9JvVKx4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the principal modes (components)\n",
        "principal_components = pca.components_.T  # Transpose to match point-wise interpretation"
      ],
      "metadata": {
        "id": "UuSxg9BrDz6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** How many principal compoenent do we have? What is the dimension of  each principal components?"
      ],
      "metadata": {
        "id": "i6QGolj3N46a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dimensions of each principal component\n",
        "print(\"Number of principal components:\", principal_components.shape)\n",
        "print(\"Dimension of each principal component:\", principal_components.shape[1])"
      ],
      "metadata": {
        "id": "8zSNBdG4OJhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** Plot the precentage of cumulative variance vs number of principal components index curve.\n",
        "\n",
        "**TIP:** To plot the cumulative variance explained by each principal component in PCA, you can use the `explained_variance_ratio_` attribute from the fitted PCA model. This attribute contains the proportion of variance explained by each principal component. The cumulative variance can be computed by taking the cumulative sum of these values (`cumsum`)."
      ],
      "metadata": {
        "id": "C2dx5fCmMZEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get explained variance ratio (how much variance each principal component explains)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Compute cumulative variance\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot cumulative variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel(\"Index of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B3BksPBiLAnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE:** How many principal components are needed to explain 90%, 95%, and 99% of the variance?\n",
        "\n",
        "**TIP:** To determine how many principal components are needed to explain 90%, 95%, and 99% of the variance, you can use the cumulative variance plot and check at which point the cumulative variance exceeds these thresholds.\n"
      ],
      "metadata": {
        "id": "npcfiYXsM4mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the number of components to reach a certain threshold\n",
        "def get_components_for_threshold(cumulative_variance, threshold):\n",
        "    # Find the first component where cumulative variance exceeds the threshold\n",
        "    return np.argmax(cumulative_variance >= threshold) + 1  # +1 because index is zero-based\n",
        "\n",
        "# Find the number of components for 90%, 95%, and 99% variance explained\n",
        "components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
        "components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of components to explain 90% variance: {components_90}\")\n",
        "print(f\"Number of components to explain 95% variance: {components_95}\")\n",
        "print(f\"Number of components to explain 99% variance: {components_99}\")"
      ],
      "metadata": {
        "id": "xHsflHN1M4R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cumulative variance for visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.axvline(x=components_90, color='r', linestyle='--', label=\"90% Variance\")\n",
        "plt.axvline(x=components_95, color='g', linestyle='--', label=\"95% Variance\")\n",
        "plt.axvline(x=components_99, color='y', linestyle='--', label=\"99% Variance\")\n",
        "plt.title(\"Cumulative Explained Variance\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9746SL0jb96M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting a PCA model to your data, you can compute the coefficients by using the transform method of the PCA object. This method projects the original data onto the principal components, effectively computing the coefficients (i.e., the contributions of each principal component for each data point)."
      ],
      "metadata": {
        "id": "lBe79VpLNzH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the coefficients (also known as the projection onto the principal components)\n",
        "principal_coefficients = pca.transform(X_centered)\n",
        "\n",
        "# Print the shape of the coefficients matrix\n",
        "print(\"Shape of coefficients matrix:\", principal_coefficients.shape)"
      ],
      "metadata": {
        "id": "lFm0wNYzNxsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can reconstruct one of the original shapes by using all the principal components obtained from PCA. The idea is to combine these components with their respective weights to approximate the original shape as closely as possible.\n",
        "\n",
        "Once we have the reconstructed shape, we compare it to the original shape to measure how well the reconstruction captures the original details. To do this, we use the Mean Squared Error (MSE). The MSE quantifies the average squared difference between the original and reconstructed values.\n",
        "\n",
        "A lower MSE indicates a better reconstruction, meaning the reconstructed shape is very close to the original. If the MSE is high, it suggests that some important details might have been lost during the reconstruction. This process helps us evaluate how effective PCA is at preserving the essential features of the data."
      ],
      "metadata": {
        "id": "m_pwO4McsAdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the first shape for reconstruction\n",
        "first_shape = X[0]  # Use the first shape for reconstruction\n",
        "\n",
        "# Reconstruct the first shape using the all the components\n",
        "reconstructed_shape = np.dot(principal_coefficients[0,], principal_components[:, :].T) + X_mean\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "print(\"\\nReconstruction Error:\")\n",
        "print(mse_error)"
      ],
      "metadata": {
        "id": "pBpQ01LdTZ6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now reconstruct the first shape using only a subset of the principal components, rather than all of them. This allows us to investigate how the number of components affects the quality of the reconstruction. By limiting the reconstruction to the most significant components, we can focus on the key patterns of variation while discarding smaller, potentially less important details.\n",
        "\n",
        "This approach is useful for dimensionality reduction and helps evaluate how well the most important components capture the essence of the data. The Mean Squared Error (MSE) will indicate how much information is lost when using fewer components."
      ],
      "metadata": {
        "id": "izvaV4Wjc687"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of principal components to use\n",
        "num_components_to_use = 10  # Replace with the desired number of components\n",
        "\n",
        "# Use the first shape for reconstruction\n",
        "first_shape = X[0]\n",
        "\n",
        "# Reconstruct the first shape using only the selected number of components\n",
        "reconstructed_shape = (\n",
        "    np.dot(principal_coefficients[0, :num_components_to_use], principal_components[:, :num_components_to_use].T) + X_mean\n",
        ")\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "print(\"\\nReconstruction Error using\", num_components_to_use, \"components:\")\n",
        "print(mse_error)"
      ],
      "metadata": {
        "id": "HlZbtVxwcr2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXERCIZE**:  Modify the code to iteratively increase the number of principal components used for reconstruction, starting from 0 and incrementing by steps of 10, up to the maximum number of components. For each iteration, compute and record the reconstruction error (using Mean Squared Error, MSE) to observe how the error decreases as more components are included. Plot the results."
      ],
      "metadata": {
        "id": "Wg1bbl5Im0wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store MSE values\n",
        "mse_errors = []\n",
        "\n",
        "# Loop over 1, 5, and 10 principal components to reconstruct the shape\n",
        "for n_components in range(1, 99, 10):  # n_components ranges from 1 to 50 with step 5\n",
        "    # Reconstruct the first shape using the first n_components\n",
        "    reconstructed_shape = np.dot(principal_coefficients[0, :n_components], principal_components[:, :n_components].T) + X_mean\n",
        "\n",
        "    # Compute the Mean Squared Error (MSE) for the reconstruction\n",
        "    mse_error = np.mean((first_shape - reconstructed_shape) ** 2)\n",
        "\n",
        "     # Store the MSE for plotting\n",
        "    mse_errors.append(mse_error)\n",
        "\n",
        "    # Print the MSE for the current reconstruction\n",
        "    print(f\"\\nReconstruction Error with {n_components} components:\")\n",
        "    print(f\"MSE: {mse_error}\")\n",
        "\n",
        "# Plotting the reconstruction errors\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, 99, 10), mse_errors, marker='o', linestyle='-', color='b', label='Reconstruction Error (MSE)')\n",
        "plt.title('Reconstruction Error vs. Number of Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0rOJwg7tm-K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXTRA"
      ],
      "metadata": {
        "id": "qOyylLFQvZCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script performs Principal Component Analysis (PCA) on 3D shapes, reconstructs the shapes using a specified number of principal components, and then visualizes the original and reconstructed shapes interactively."
      ],
      "metadata": {
        "id": "KcVbfLSpdlc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Function to visualize 3D meshes interactively using Plotly\n",
        "def visualize_3d_mesh(original_points, reconstructed_points):\n",
        "    \"\"\"\n",
        "    Visualizes the original and reconstructed 3D meshes interactively using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "        original_points (numpy.ndarray): Points of the original 3D shape.\n",
        "        reconstructed_points (numpy.ndarray): Points of the reconstructed 3D shape.\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add the original shape as blue points for reference\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=original_points[:, 0],\n",
        "        y=original_points[:, 1],\n",
        "        z=original_points[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color='blue', opacity=0.6),\n",
        "        name='Original Shape'\n",
        "    ))\n",
        "\n",
        "    # Add the reconstructed shape as red points for comparison\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=reconstructed_points[:, 0],\n",
        "        y=reconstructed_points[:, 1],\n",
        "        z=reconstructed_points[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=2, color='red', opacity=0.6),\n",
        "        name='Reconstructed Shape'\n",
        "    ))\n",
        "\n",
        "    # Set layout options including title and axis labels for clarity\n",
        "    fig.update_layout(\n",
        "        title=\"3D Shape Visualization (Original and Reconstructed)\",\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X\",\n",
        "            yaxis_title=\"Y\",\n",
        "            zaxis_title=\"Z\"\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)\n",
        "    )\n",
        "\n",
        "    # Display the interactive 3D plot\n",
        "    fig.show()\n",
        "\n",
        "# Example usage: Perform PCA reconstruction and visualization for a specific shape\n",
        "which_shape = 2  # Index of the shape to process (you can change this to visualize a different shape)\n",
        "n_components = 10  # Number of principal components to use for the reconstruction\n",
        "\n",
        "# Calculate the mean of the dataset (used for PCA reconstruction)\n",
        "X_mean  = np.mean(X, axis=0)\n",
        "\n",
        "# Load the shape data (this assumes X is your dataset of 3D shapes, where each shape is a vector)\n",
        "# Original shape data for a given shape index 'which_shape'\n",
        "original_shape = X[which_shape]  # Access the shape data (e.g., X[0] for the first shape)\n",
        "\n",
        "# Reshape the original shape into a 3D point format (assuming 3D points: x, y, z)\n",
        "n_points = original_shape.shape[0] // 3  # Assuming each shape has 3D points (x, y, z)\n",
        "original_shape = original_shape.reshape(n_points, 3)\n",
        "\n",
        "# Perform PCA reconstruction using the selected number of components\n",
        "reconstructed_shape = np.dot(principal_coefficients[which_shape, :n_components], principal_components[:, :n_components].T) + X_mean\n",
        "\n",
        "# Reshape the reconstructed data back into a 3D point format\n",
        "n_points = reconstructed_shape.shape[0] // 3  # Reshape back to 3D coordinates\n",
        "reconstructed_shape = reconstructed_shape.reshape(n_points, 3)\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) for the reconstruction accuracy\n",
        "mse_error = np.mean((original_shape - reconstructed_shape) ** 2)\n",
        "print(f\"\\nReconstruction Error (MSE) using {n_components} components:\")\n",
        "print(mse_error)\n",
        "\n",
        "# Visualize the original and reconstructed shapes\n",
        "visualize_3d_mesh(original_shape, reconstructed_shape)"
      ],
      "metadata": {
        "id": "6qzpOpAZvZ7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code shows how the application of PCA modes (principal components) can modify a mean 3D shape to generate different variations. The goal of the code is to apply PCA to a set of 3D shapes, calculate their mean shape, and then modify the mean shape by applying a specific PCA mode (principal component). Each mode represents a different direction of variation in the shape, and by adjusting the strength of this variation, we can see how the shape changes."
      ],
      "metadata": {
        "id": "QAKkLt_-j4pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Function to visualize 3D meshes interactively using Plotly\n",
        "def visualize_3d_mesh(mean_shape, modified_shape):\n",
        "    \"\"\"\n",
        "    Visualizes the original mean shape and the modified shape after applying a PCA mode interactively using Plotly.\n",
        "\n",
        "    Parameters:\n",
        "        mean_shape (numpy.ndarray): The mean 3D shape.\n",
        "        modified_shape (numpy.ndarray): The modified shape after applying a specific PCA mode.\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add the mean shape as blue points for reference\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=mean_shape[:, 0],  # x-coordinates of the mean shape\n",
        "        y=mean_shape[:, 1],  # y-coordinates of the mean shape\n",
        "        z=mean_shape[:, 2],  # z-coordinates of the mean shape\n",
        "        mode='markers',  # Scatter plot with points\n",
        "        marker=dict(size=2, color='blue', opacity=0.6),  # Blue points with some transparency\n",
        "        name='Mean Shape'  # Label for the mean shape in the legend\n",
        "    ))\n",
        "\n",
        "    # Add the modified shape (after applying PCA mode) as green points for comparison\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=modified_shape[:, 0],  # x-coordinates of the modified shape\n",
        "        y=modified_shape[:, 1],  # y-coordinates of the modified shape\n",
        "        z=modified_shape[:, 2],  # z-coordinates of the modified shape\n",
        "        mode='markers',  # Scatter plot with points\n",
        "        marker=dict(size=2, color='green', opacity=0.6),  # Green points with some transparency\n",
        "        name='Modified Shape (Mode Applied)'  # Label for the modified shape in the legend\n",
        "    ))\n",
        "\n",
        "    # Set layout options, including title and axis labels for better clarity\n",
        "    fig.update_layout(\n",
        "        title=\"3D Shape Visualization (Mean Shape and Modified with PCA Mode)\",  # Title of the plot\n",
        "        scene=dict(\n",
        "            xaxis_title=\"X\",  # Label for the X-axis\n",
        "            yaxis_title=\"Y\",  # Label for the Y-axis\n",
        "            zaxis_title=\"Z\"   # Label for the Z-axis\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=30)  # Margin settings to optimize the plot's appearance\n",
        "    )\n",
        "\n",
        "    # Display the interactive 3D plot\n",
        "    fig.show()\n",
        "\n",
        "# Calculate the mean of the dataset X (this is the average 3D shape used for PCA reconstruction)\n",
        "X_mean  = np.mean(X, axis=0)  # Compute the mean of all shapes along each dimension\n",
        "\n",
        "# User selects which PCA mode (principal component) to apply to the mean shape\n",
        "selected_mode = 0  # Index of the selected PCA mode to visualize (0-indexed)\n",
        "\n",
        "# Calculate the standard deviation of the coefficients for the selected mode\n",
        "# principal_coefficients should be the coefficients from the PCA (this is usually X @ principal_components)\n",
        "mode_std = np.std(principal_coefficients[:, selected_mode])  # Standard deviation of the coefficients for the selected mode\n",
        "print(f\"Standard deviation of the selected mode: {mode_std}\")\n",
        "\n",
        "# Define the coefficient to multiply by the standard deviation to adjust the strength of the modification\n",
        "coeff = -2.0  # Coefficient for scaling the standard deviation (this is user-controlled)\n",
        "\n",
        "# Modify the mean shape by adding the selected PCA mode's contribution, scaled by the coefficient and mode's standard deviation\n",
        "# The principal component is reshaped to match the flattened mean shape\n",
        "principal_components = principal_components.T  # Transpose the principal components for easier access\n",
        "X_modified = X_mean + coeff * mode_std * principal_components[selected_mode, :]  # Scale the mode's effect\n",
        "\n",
        "# Reshape the flattened mean shape back into 3D point format (assuming each shape has 3D points: x, y, z)\n",
        "n_points = X_mean.shape[0] // 3  # Number of 3D points in the shape (each point has 3 coordinates: x, y, z)\n",
        "shape_mean = X_mean.reshape(n_points, 3)  # Reshape the mean shape into a 3D array\n",
        "modified_shape = X_modified.reshape(n_points, 3)  # Reshape the modified shape into a 3D array\n",
        "\n",
        "# Visualize the mean shape and the modified shape after applying the selected PCA mode\n",
        "visualize_3d_mesh(shape_mean, modified_shape)\n"
      ],
      "metadata": {
        "id": "wmDXx70wf2g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3nApnA9kkM0d"
      }
    }
  ]
}